---
title: "多模态 Agent 时代来了：从 Qwen3.5 看 AI 助手的进化"
date: 2026-02-17
description: "Qwen3.5 发布原生多模态 Agent 能力，这意味着什么？AI 助手正在从「能看」进化到「能操作」。"
tags: ["AI", "Agent", "多模态", "Qwen"]
---

阿里的 Qwen 团队刚发布了 Qwen3.5，主打「原生多模态 Agent」能力。这不是简单的「加个视觉模块」——而是从架构层面重新思考 AI 助手应该怎么与世界交互。

## 当前 Agent 的瓶颈

大多数 AI Agent 架构是这样的：

```
用户请求 → LLM 理解 → 调用工具 → 返回结果
```

问题在于「调用工具」这一步。现有的工具接口都是 **文本化** 的：

- 想操作网页？要解析 DOM 或 accessibility tree
- 想填表单？要找到 element ID 或 CSS selector
- 想点击按钮？要知道确切的坐标或元素名

这就像是让一个人戴着眼罩，只能通过别人的文字描述来操作电脑。

## 「原生多模态」的意义

Qwen3.5 的思路是：让模型直接「看」屏幕，然后「指」向它想操作的地方。

这解决了几个实际问题：

### 1. 不再依赖 DOM 解析

很多现代网页用 canvas 渲染、用 shadow DOM、用各种奇怪的框架。传统的 accessibility tree 方法经常失效。

但视觉模型不在乎这些——它看到的就是用户看到的。

### 2. 跨平台一致性

同一个模型可以操作：
- 网页
- 桌面应用
- 移动端
- 甚至游戏界面

因为它只需要「看图说话」，不需要针对每个平台写适配器。

### 3. 更自然的错误恢复

当操作失败时，传统 agent 的日志是：

```
Error: Element #submit-btn not found
```

而视觉 agent 可以「看一眼」，然后说：

> 页面上有个「提交」按钮，但好像是灰色的，可能需要先填完表单。

这种反馈对调试和用户理解都更友好。

## 现实中的挑战

当然，原生多模态 agent 也有自己的问题：

**延迟**：处理图像比处理文本慢。每次「看一眼」都需要编码整个屏幕。

**成本**：图像 token 比文本 token 贵得多。如果 agent 每秒都要「看」屏幕，成本会爆炸。

**准确性**：指向「这个按钮」听起来简单，但像素级的定位仍然容易出错。

**隐私**：让 AI 「看」你的屏幕，意味着把屏幕内容发送到服务器。

## 对独立开发者的启示

如果你在做 AI 产品，这波趋势值得关注：

1. **MCP (Model Context Protocol) 依然重要**：视觉理解很强，但精确的 API 调用仍然更可靠。两者互补。

2. **UI 设计要「AI 友好」**：清晰的视觉层次、明确的按钮标签，不只是帮人类用户，也帮 AI 理解。

3. **考虑「观察模式」**：很多场景下，AI 只需要「看」而不需要「操作」。比如监控仪表盘、读取数据。

4. **本地推理有优势**：隐私敏感场景下，本地运行的多模态模型会更有吸引力。

## 下一步会是什么？

我的预测：

- **2026 下半年**：主流 coding agent 会支持「看屏幕」来 debug UI 问题
- **2027**：自主 web agent 成为标配（不再需要预设的 scraper）
- **再往后**：物理机器人 + 多模态 AI = 真正的具身智能

Qwen3.5 是这个方向的一个里程碑。不管你用不用 Qwen，这个趋势值得关注。

---

*你在用什么 Agent 工具？欢迎在 [Twitter](https://twitter.com/indiekitai) 或 [HN](https://news.ycombinator.com/item?id=47014058) 讨论。*
